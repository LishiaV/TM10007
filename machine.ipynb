{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: TM10007 - Machine learning\n",
    "Editors: Lishia Vergeer, Amy Roos, Maaike Pruijt, Hilde Roording.\n",
    "\n",
    "Description: The aim of this code is to predict the tumor grade of gliomaâ€™s(high or low) before surgery, \n",
    "based on features extracted from a combination of four MRI images: \n",
    "T2-weighted, T2-weighted FLAIR and T1-weighted before and after injection of contrast agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hilde\\miniconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# General packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "\n",
    "# Import code\n",
    "from brats.load_data import load_data\n",
    "\n",
    "# Performance \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition\n",
    "import seaborn\n",
    "\n",
    "\n",
    "# Pipeline and gridsearch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# scaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Machine learning classifiers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import feature_selection \n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in data_brats: 167\n",
      "The number of columns in data_brats: 725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hilde\\Python\\TM10007_Machine_Learning\\TM10007\\brats\\load_data.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append(data2)\n"
     ]
    }
   ],
   "source": [
    "data_brats = load_data()\n",
    "\n",
    "# Convert to dataframe\n",
    "X = pd.DataFrame(data_brats)\n",
    "\n",
    "# Print data \n",
    "print(f'The number of samples in data_brats: {len(X.index)}')\n",
    "print(f'The number of columns in data_brats: {len(X.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data in X and y\n",
    "Split in X (data) and y (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split column label from dataset X\n",
    "y = X.pop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data in train and test set\n",
    "This function creates a panda dataframe and splits the data into test and train components.\n",
    "This is done with test_size variable and the function train_test_split from the sklearn module.\n",
    "Returns a train set with the data of 80% and a test set of 20% of the subjects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infinity to NaN\n",
    "X_train[X_train==np.inf]=np.nan\n",
    "X_test[X_test==np.inf]=np.nan\n",
    "\n",
    "# non-numeric features to NaN\n",
    "X_train = X_train.replace(['#DIV/0!'], np.nan)\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "X_test  = X_test.replace(['#DIV/0!'], np.nan)\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# If the total number of NaN observations in a column are greater than 40%, delete the entire column.\n",
    "perc = 40.0\n",
    "min_count = int(((100-perc)/100)*X_train.shape[0] + 1)\n",
    "X_train_drop = X_train.dropna(axis=1, thresh=min_count)\n",
    "X_labels = X_train_drop.keys()\n",
    "\n",
    "X_test_drop = X_test[X_labels]\n",
    "\n",
    "# fill the NaN observations.\n",
    "data_fill_train = X_train_drop.fillna(X_train_drop.mean()) \n",
    "data_fill_test = X_test_drop.fillna(X_test_drop.mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robustscaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(data_fill_train)\n",
    "X_train_scaled = scaler.transform(data_fill_train)\n",
    "X_test_scaled = scaler.transform(data_fill_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a PCA\n",
    "pca = decomposition.PCA(n_components=5)\n",
    "pca.fit(X_train_scaled) \n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "classifiers = (KNeighborsClassifier(), RandomForestClassifier(), SVC(kernel=\"rbf\", C=0.025, probability=True), NuSVC(probability=True))\n",
    "\n",
    "for cls in classifiers:\n",
    "    model = cls\n",
    "    model.fit(X_train_pca, y_train)\n",
    "\n",
    "    preds = model.predict(X_train_pca)\n",
    "\n",
    "    metrics.plot_confusion_matrix(model, X_train_pca, y_train)\n",
    "    display_labels =['Negative', 'Positive']\n",
    "\n",
    "\n",
    "    confusion = metrics.confusion_matrix(y_train, preds)\n",
    "    accuracy = metrics.accuracy_score(y_train, preds)\n",
    "    sensitivity = metrics.recall_score(y_train, preds, pos_label='GBM')\n",
    "    specificity = metrics.recall_score(y_train, preds, pos_label='LGG')\n",
    "\n",
    "    #table = {'classifier': cls, 'accuracy': accuracy, 'Sensitivity', sensitivity, 'Specificity': specificity}\n",
    "    print(f'For classifier: {cls}, Accuracy: {accuracy}, Sensitivity: {sensitivity}, Specificity{specificity}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-validation object\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle= True, random_state = 1)\n",
    "\n",
    "X = X_train_pca\n",
    "\n",
    "y = y_train.values\n",
    "y = np.where(y=='GBM', 1, y)\n",
    "y = np.where(y=='LGG', 2, y)\n",
    "y = y.tolist()\n",
    "\n",
    "classifiers = (KNeighborsClassifier(), RandomForestClassifier(), SVC(kernel=\"rbf\", C=0.025, probability=True), NuSVC(probability=True), DecisionTreeClassifier() )\n",
    "\n",
    "score_dict = {}\n",
    "for cls in classifiers:\n",
    "\n",
    "    list_scores = cross_val_score(cls, X, y, scoring = 'accuracy', cv=cv)\n",
    "    mean = np.mean(list_scores)\n",
    "    #print(f'The accuracy of {cls} is: {list_scores}')\n",
    "    print(f'The mean accuracy of classifier {cls}: {mean}')\n",
    "    score_dict[cls] = mean\n",
    "\n",
    "best_performing = max(score_dict, key=score_dict.get)\n",
    "value_best_performing = score_dict[best_performing]\n",
    "print(f'The best performing classifier is {best_performing} with mean value {value_best_performing}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest model with hyperparameters \n",
    "First define some hyperparameters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [10, 17, 25, 33, 41, 48, 56, 64, 72, 80], 'max_features': ['auto', 'sqrt'], 'max_depth': [2, 4], 'min_sample_split': [2, 5], 'min_sample_leaf': [1, 2], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree \n",
    "#max_depth = [2, 4]\n",
    "# Minimum number of samples required to split a node\n",
    "#min_sample_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "#min_sample_leaf = [1, 2]\n",
    "# Method of selecting samples for training each tree\n",
    "#bootstrap = [True, False]\n",
    "\n",
    "# Create a dictionary of the hyperparameters above (create a parameter grid)\n",
    "param_grid = {\n",
    "    'n_estimators': n_estimators, \n",
    "    'max_features': max_features} \n",
    "\n",
    "print(param_grid)\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': n_estimators, \n",
    "#     'max_features': max_features, \n",
    "#     'max_depth':max_depth, \n",
    "#     'min_sample_split': min_sample_split, \n",
    "#     'min_sample_leaf':min_sample_leaf, \n",
    "#     'bootstrap': bootstrap }\n",
    "# print(param_grid)\n",
    "\n",
    "# Create an insance of the classifier\n",
    "rf_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a Gridsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the RF model, take the hyperparameters, cv = amount of folds, \n",
    "#rf_grid = GridSearchCV(estimator = rf_model, param_grid = param_grid, cv = 10, verbose = 2, n_jobs = 4)\n",
    "#rf_grid.fit(X_train_pca, y_train)\n",
    "#rf_grid.best_params_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "# Doing a random grid search \n",
    "# A randomised grid search will perform worse than a gridsearch but it is faster \n",
    "rf_random_grid = RandomizedSearchCV(estimator = rf_model, param_distributions = param_grid, cv = 10, verbose = 2, n_jobs = 4)\n",
    "rf_random_grid.fit(X_train_pca, y_train)\n",
    "rf_random_grid.best_params_\n",
    "\n",
    "#print (f' Train Accuracy: {rf_grid.score(X_train_pca, y_train):.3f}')\n",
    "#print (f' Test Accuracy: {rf_grid.score(X_test_pca, y_test):.3f}')\n",
    "\n",
    "print (f' Test Accuracy: {rf_random_grid.score(X_train_pca, y_train):.3f}')\n",
    "print (f' Test Accuracy: {rf_random_grid.score(X_test_pca, y_test):.3f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75db1649061a4efd24d53350561334aa22b1bfcd544c077d6d1bdd9329699d44"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
