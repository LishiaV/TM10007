{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: TM10007 - Machine learning\n",
    "Editors: Lishia Vergeer, Amy Roos, Maaike Pruijt, Hilde Roording.\n",
    "\n",
    "Description: The aim of this code is to predict the tumor grade of gliomaâ€™s (high or low), \n",
    "based on features extracted from a combination of four MRI images: \n",
    "T2-weighted, T2-weighted FLAIR and T1-weighted before and after injection of contrast agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/LishiaV/TM10007.git\n",
    "\n",
    "# General packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# Import code\n",
    "from TM10007.brats.load_data import load_data\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# scaler\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# gridsearch\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Machine learning classifiers\n",
    "from sklearn import decomposition, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "In nthe section below the data is loaded with given function load_data.py by creating a dataframe from de CSV file called data_brats.csv. The number of samples and features is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_brats = load_data()\n",
    "\n",
    "# Convert to dataframe\n",
    "X = pd.DataFrame(data_brats)\n",
    "\n",
    "# Print data \n",
    "print(f'The number of samples in data_brats: {len(X.index)}')\n",
    "print(f'The number of columns in data_brats: {len(X.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data in X (data) and y (label)\n",
    "The data contains the columns with features as well as a column with the label that needs to be predicted (GBM or LGG). In this section this label is splitted from the dataframe in X (data) and y (label). Also the textual labels are converted to binary numbers to be easily processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split column label from dataset X\n",
    "y = X.pop('label')\n",
    "\n",
    "# replace 'GBM' and 'LGG' with 1 and 0 respectively\n",
    "y = y.replace(['GBM'], 1)\n",
    "y = y.replace(['LGG'], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data in train and test set\n",
    "This section splits the data into two data frames, a test and a train data frame with the train_test_split function from the sklearn module.\n",
    "This returns a train set with the data of 80% and a test set of 20% of the subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divergent values to NaN\n",
    "In this section non-numeric values as 'infinity' and '#DIV/0' values in both the test and train data frames are replaced by 'NaN' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infinity to NaN\n",
    "X_train[X_train==np.inf]=np.nan\n",
    "X_test[X_test==np.inf]=np.nan\n",
    "\n",
    "# non-numeric features to NaN\n",
    "X_train = X_train.replace(['#DIV/0!'], np.nan)\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "X_test  = X_test.replace(['#DIV/0!'], np.nan)\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize missing data\n",
    "In this section below the missing values of the training data frame are visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sb.heatmap(X_train.isna().transpose(),\n",
    "            yticklabels=False, xticklabels=False, \n",
    "            cmap=\"YlGnBu\",\n",
    "            cbar_kws={'label': 'Missing Data'})\n",
    "plt.savefig(\"visualizing_missing_data_with_heatmap_Seaborn_Python.png\", dpi=100)\n",
    "plt.title('Heatmap missing values')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('ID')\n",
    "plt.savefig(\"visualizing_missing_data_with_heatmap.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set outliers to NaN\n",
    "In the section below the outliers of the train data frame are converted to 'NaN' values. Below the amount of outliers is visualized in a table and a histogram.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_outs = []\n",
    "count_percent = []\n",
    "for column in X_train:\n",
    "    # Removing outliers\n",
    "    q1 = X_train[column].quantile(0.25)\n",
    "    q3 = X_train[column].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "\n",
    "    # Information outliers\n",
    "    outliers = [x for x in X_train[column] if x < lower or x > upper]\n",
    "    outliers_removed = [x for x in X_train[column] if x >= lower and x <= upper]\n",
    "    count_out = float(len(outliers))\n",
    "    count_rem = float(len(outliers_removed))\n",
    "    count_outs.append(count_out)\n",
    "\n",
    "    if not count_out and not count_rem:\n",
    "        percent = 0    \n",
    "    else:\n",
    "        percent = count_out/(count_out+count_rem)*100\n",
    "    count_percent.append(percent)\n",
    "\n",
    "    # Outliers to NaN\n",
    "    X_train[column].loc[X_train[column] > upper] = np.nan\n",
    "    X_train[column].loc[X_train[column] < lower] = np.nan\n",
    "\n",
    "# percent\n",
    "plt.figure\n",
    "plt.hist(x=count_percent, bins='auto', alpha=0.7, rwidth=0.85)\n",
    "plt.title('Detected as outliers')\n",
    "plt.xlabel('Part detected [%]')\n",
    "plt.ylabel('Amount of features')\n",
    "plt.savefig(\"Removed.png\", dpi=100)\n",
    "df_describe = pd.DataFrame(count_percent)\n",
    "df_describe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising missing data after removing outliers\n",
    "In the section below the training data frame is visualized after the removal of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sb.heatmap(X_train.isna().transpose(),\n",
    "            yticklabels=False, xticklabels=False, \n",
    "            cmap=\"YlGnBu\",\n",
    "            cbar_kws={'label': 'Missing Data'})\n",
    "plt.savefig(\"visualizing_missing_data_with_heatmap_Seaborn_Python.png\", dpi=100)\n",
    "plt.title('Heatmap missing values after removing outliers')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('ID')\n",
    "plt.savefig(\"visualizing_missing_data_with_heatmap_outliers.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop insufficient columnns (features)\n",
    "In the section below all features that contain more than 40% NaN values in de train data frame are dropped from the data frame. The same features are also dropped from the test data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the total number of NaN observations in a column are greater than 40%, delete the entire column.\n",
    "perc = 40.0\n",
    "min_count = int(((100-perc)/100)*X_train.shape[0] + 1)\n",
    "X_train_drop = X_train.dropna(axis=1, thresh=min_count)\n",
    "X_labels = X_train_drop.keys()\n",
    "\n",
    "X_test_drop = X_test[X_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill the NaN values.\n",
    "\n",
    "In the section below all remaining NaN values are replaced by the mean of the values from the corresponding feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fill_train = X_train_drop.fillna(X_train_drop.mean()) \n",
    "data_fill_test = X_test_drop.fillna(X_test_drop.mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale features\n",
    "In the section below the train data as well as the test data is normalized by the use of the MinMaxScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_fill_train)\n",
    "X_train_scaled = scaler.transform(data_fill_train)\n",
    "X_test_scaled = scaler.transform(data_fill_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform features\n",
    "To create a dataframe with usefull features to train the classifier, a principe component analysis (PCA) is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation PCA components variance\n",
    "pca_visual = decomposition.PCA(n_components=50)\n",
    "pca_visual.fit(X_train_scaled) \n",
    "explained_variance = pca_visual.explained_variance_ratio_\n",
    "plt.plot(explained_variance)\n",
    "plt.title('Variance of PCA principle components')\n",
    "plt.xlabel('PC')\n",
    "plt.ylabel('Variance')\n",
    "plt.savefig(\"Variance.png\", dpi=100)\n",
    "\n",
    "# Perform PCA\n",
    "pca = decomposition.PCA(n_components=10)\n",
    "pca.fit(X_train_scaled) \n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation\n",
    "In the section below the cross validation score (specifically the ROC AUC) of three classifiers is calculated, based on which the best classifier can be selected for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-validation object\n",
    "cv = KFold(n_splits=5, shuffle= True, random_state = 1)\n",
    "\n",
    "X = X_train_pca\n",
    "\n",
    "y = y_train.values\n",
    "y = y.tolist()\n",
    "\n",
    "classifiers = (KNeighborsClassifier(), RandomForestClassifier(), SVC())\n",
    "\n",
    "score_list = []\n",
    "for cls in classifiers:\n",
    "    rocauc = cross_val_score(cls, X, y, scoring = 'roc_auc', cv = cv)\n",
    "    mean = np.mean(rocauc)\n",
    "    score_list.append(mean)\n",
    "\n",
    "df = pd.DataFrame(score_list, index =['KNN', 'RF', 'SVM'], columns =['mean ROC AUC'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine hyper parameter search \n",
    "In the section below the optimal settings for the selected classifier (SVM) are calculated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma' : [1, 0.1, 0.01, 0.001],\n",
    "    'kernel' : ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "svm_model = SVC(gamma='auto')\n",
    "\n",
    "svm_grid = RandomizedSearchCV(estimator = svm_model, param_distributions = param_grid, cv = 5, verbose = 2, n_jobs =4)\n",
    "svm_grid.fit(X_train_pca, y_train)\n",
    "\n",
    "best_estimator = svm_grid.best_estimator_\n",
    "\n",
    "print(f'The best estimators are : {best_estimator}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance SVM\n",
    "In the section below the performance of the SVM classifier used for the test set is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svm_grid.predict(X_test_pca)\n",
    "metrics.ConfusionMatrixDisplay.from_predictions(y_test, pred)\n",
    "plt.show()\n",
    "\n",
    "classifier = 'SVM'\n",
    "accuracy = metrics.accuracy_score(y_test, pred)\n",
    "sensitivity = metrics.recall_score(y_test, pred, pos_label=1)\n",
    "specificity = metrics.recall_score(y_test, pred, pos_label=0)\n",
    "F1 = metrics.f1_score(y_test, pred, average='micro')\n",
    "\n",
    "dict_test = {'accuracy': [accuracy], 'sensitivity': [sensitivity], 'specificity': [specificity], 'F1': [F1]}\n",
    "performance = pd.DataFrame(dict_test, index = ['SVM classifier'])\n",
    "\n",
    "print(performance)\n",
    "print(f'Test accuracy train set is:{svm_grid.score(X_train_pca, y_train):.3f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75db1649061a4efd24d53350561334aa22b1bfcd544c077d6d1bdd9329699d44"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
