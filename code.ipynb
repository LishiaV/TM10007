{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: TM10007 - Machine learning\n",
    "Editors: Lishia Vergeer, Amy Roos, Maaike Pruijt, Hilde Roording.\n",
    "\n",
    "Description: The aim of this code is to predict the tumor grade of gliomaâ€™s(high or low) before surgery, \n",
    "based on features extracted from a combination of four MRI images: \n",
    "T2-weighted, T2-weighted FLAIR and T1-weighted before and after injection of contrast agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "\n",
    "# Import code\n",
    "from brats.load_data import load_data\n",
    "\n",
    "# Performance \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition\n",
    "\n",
    "# Pipeline and gridsearch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# scaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Machine learning classifiers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import feature_selection \n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in data_brats: 167\n",
      "The number of columns in data_brats: 725\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "from brats.load_data import load_data\n",
    "\n",
    "data_brats = load_data()\n",
    "\n",
    "# Convert to dataframe\n",
    "X = pd.DataFrame(data_brats)\n",
    "\n",
    "print(f'The number of samples in data_brats: {len(X.index)}')\n",
    "print(f'The number of columns in data_brats: {len(X.columns)}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOLUME_ET        int64\n",
      "VOLUME_NET       int64\n",
      "VOLUME_ED        int64\n",
      "VOLUME_TC        int64\n",
      "VOLUME_WT        int64\n",
      "                ...   \n",
      "TGM_Cog_X_6    float64\n",
      "TGM_Cog_Y_6    float64\n",
      "TGM_Cog_Z_6    float64\n",
      "TGM_T_6        float64\n",
      "label           object\n",
      "Length: 725, dtype: object\n",
      "object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# Check datatypes\n",
    "print(X.dtypes)\n",
    "print(X.dtypes['VOLUME_ET_OVER_ED'])\n",
    "print(X.dtypes['VOLUME_NET_OVER_ED'])\n",
    "# both categorical and numeric variables\n",
    "# die twee kolommen gaven een error omdat het objecten zijn! dus daarom verderop eruit gehaald."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data in X and y\n",
    "Split in X (data) and y (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in y: 167\n"
     ]
    }
   ],
   "source": [
    "# split column label from dataset X\n",
    "y = X.pop('label')\n",
    "print(f'The number of samples in y: {len(y.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data in train and test set\n",
    "This function creates a panda dataframe and splits the data into test and train components.\n",
    "This is done with test_size variable and the function train_test_split from the sklearn module.\n",
    "Returns a train set with the data of 80% and a test set of 20% of the subjects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing : \n",
    "###### deze stap moet binnen de pipeline komen denk ik? en die NaN stappen van de oude code ook!\n",
    "###### moet hier onder ook nog een verwerking van test data komen? daar kunnen ook #div/0 in zitten toch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infinity to NaN\n",
    "X_train[X_train==np.inf]=np.nan\n",
    "\n",
    "# non-numeric features to NaN\n",
    "X_train = X_train.replace(['#DIV/0!'], np.nan)\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "##### building a pipeline to define each transformer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputation', SimpleImputer(missing_values = np.NaN, strategy='most_frequent')),   # kan ook strategy = 'most frequent'\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use the ColumnTransformer to apply the transformations to the correct columns in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "#categorical_features = X_train.select_dtypes(include=['object']).drop(['VOLUME_NET_OVER_ED', 'VOLUME_ET_OVER_ED'], axis=1).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)]) # ('cat', categorical_transformer,categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_regression.py\", line 213, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 400, in _fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse=\"csr\", multi_output=True)\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 746, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 1993, in __array__\n",
      "    return np.asarray(self._values, dtype=dtype)\n",
      "ValueError: could not convert string to float: '#DIV/0!'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_regression.py\", line 213, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 400, in _fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse=\"csr\", multi_output=True)\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"C:\\Users\\amymy\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle= True, random_state = 1)\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "r_2s = cross_val_score(knn, X_train, y_train, scoring = 'r2', cv=kf)\n",
    "avg_r2 = np.mean(r_2s)\n",
    "\n",
    "print(r_2s)\n",
    "print(avg_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop some classifiers and check performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=3)\n",
      "model score: 0.765\n",
      "SVC(C=0.025, probability=True)\n",
      "model score: 0.706\n",
      "NuSVC(probability=True)\n",
      "model score: 0.824\n",
      "DecisionTreeClassifier()\n",
      "model score: 0.765\n",
      "RandomForestClassifier()\n",
      "model score: 0.824\n",
      "AdaBoostClassifier()\n",
      "model score: 0.765\n",
      "GradientBoostingClassifier()\n",
      "model score: 0.824\n"
     ]
    }
   ],
   "source": [
    "# Beste classifier gebruiken! Dit gebruiken om keuze te onderbouwen\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()]\n",
    "    \n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparametersearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter suggestions\n",
    "param_grid = { \n",
    "    'classifier__n_estimators': [200, 500],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'classifier__max_depth' : [4,5,6,7,8]}                        # kan ook nog, maar deed het niet: 'classifier__criterion' :['gini', 'entropy']\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Gridsearch with 5-fold cross validation\n",
    "Gridsearch_CV = GridSearchCV(pipe, param_grid, n_jobs= 1, cv=5)   # Hier kunnen we kiezen voor GridSearh of randomgridsearch\n",
    "                  \n",
    "Gridsearch_CV.fit(X_train, y_train)  \n",
    "\n",
    "#Kijken welke parameters het best zijn en die uiteindelijk gebruiken!\n",
    "print(Gridsearch_CV.best_params_)    \n",
    "print(Gridsearch_CV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipeline with best parameters and best classifier : voor nu gekozen voor RF \n",
    "# Error bij andere classifiers en niet bij xgb?  .XGBClassifier opzoeken bij RF enzo\n",
    "hyperparams_after_gridsearch= Gridsearch_CV.best_params_\n",
    "params_after_grid = { **static_params, **hyperparams_after_gridsearch}\n",
    "pipe_after_grid = Pipeline([('classifier', xgb.XGBClassifier(**params_after_grid))])\n",
    "\n",
    "#fit pipe with hyperparameters on complete train set\n",
    "bst= pipe_after_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score_train = roc_auc_score(y_train, bst.predict_proba(X_train)[:, 1])\n",
    "print(score_train)\n",
    "\n",
    "#TEST WERKT NOG NIET\n",
    "#score_test = roc_auc_score(y_test, bst.predict_proba(X_test)[:, 1])\n",
    "# print(score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUDE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in data_brats: 167\n",
      "The number of columns in data_brats: 724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maaik\\Desktop\\TM10007_Machine_learning\\TM10007_ML\\TM10007\\brats\\load_data.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append(data2)\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "from brats.load_data import load_data\n",
    "\n",
    "data_brats = load_data()\n",
    "print(f'The number of samples in data_brats: {len(X.index)}')\n",
    "print(f'The number of columns in data_brats: {len(X.columns)}')\n",
    "\n",
    "# Convert to dataframe\n",
    "X = pd.DataFrame(data_brats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data in X and y\n",
    "Split in X (data) and y (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in y: 167\n"
     ]
    }
   ],
   "source": [
    "# split column label from dataset X\n",
    "y = X.pop('label')\n",
    "print(f'The number of samples in y: {len(y.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data in train and test set\n",
    "This function creates a panda dataframe and splits the data into test and train components.\n",
    "This is done with test_size variable and the function train_test_split from the sklearn module.\n",
    "Returns a train set with the data of 80% and a test set of 20% of the subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No None\n",
    "Deleting columns with NaN or filling them.\n",
    "- Bepalen waar threshold ligt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight in the data\n",
    "#print(f'OVERZICHT: {X_train.isnull().sum()}')\n",
    "\n",
    "# infinity to NaN\n",
    "X_train[X_train==np.inf]=np.nan\n",
    "\n",
    "# non-numeric features to NaN\n",
    "X_train = X_train.replace(['#DIV/0!'], np.nan)\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# If the total number of NaN observations in a column are greater than 40%, delete the entire column.\n",
    "perc = 40.0\n",
    "min_count = int(((100-perc)/100)*X_train.shape[0] + 1)\n",
    "data_dropcolumn = X_train.dropna(axis=1, thresh=min_count)\n",
    "\n",
    "# fill the NaN observations.\n",
    "data_fill = data_dropcolumn.fillna(data_dropcolumn.median()) #Bekijken mean of median\n",
    "\n",
    "# Inzicht in data\n",
    "#print(f'OVERZICHT NONONE: {data_fill.isnull().sum()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.35729482  1.85343746  1.56015075 ... -0.93128157  0.21975955\n",
      "  -0.12371608]\n",
      " [ 0.89552576 -0.2814291   0.62961102 ...  0.35519775  0.65292389\n",
      "  -0.03776545]\n",
      " [ 0.4599639  -0.0106383   0.50867244 ... -0.1861754  -0.6246499\n",
      "   0.53998392]\n",
      " ...\n",
      " [ 0.13535555  2.92167015 -0.35408497 ... -0.81228628  0.35661828\n",
      "   1.50297563]\n",
      " [-0.4083876   0.30029517 -0.87507739 ... -0.41067211 -1.18149254\n",
      "  -0.11757403]\n",
      " [-0.34859132  3.49335875  0.00411862 ... -0.60665591  0.09969611\n",
      "   1.36893765]]\n"
     ]
    }
   ],
   "source": [
    "# robustscaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(data_fill)\n",
    "X_scaled = scaler.transform(data_fill)\n",
    "\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform features\n",
    "- We denken alleen PCA te gebruiken. Klopt het dat je dan niet ook selectie gebruikt?\n",
    "- PCA gaat uit van lineair model. Hoe kunnen we weten of ons onze data daar geschikt voor is?\n",
    "- Is het de bedoeling dat we ons hier verder in verdiepen of valt dat buiten de scope van het vak?\n",
    "- Uitzoeken hoe we de X_test en y_test correct gereed krijgen voor PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a PCA\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X_scaled) \n",
    "X_train_pca = pca.transform(X_scaled)\n",
    "\n",
    "#X_test_pca = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier: kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit kNN\n",
    "# knn = neighbors.KNeighborsClassifier(n_neighbors=15)\n",
    "# knn.fit(X_train_pca, y_train)\n",
    "# score_train = knn.score(X_train_pca, y_train)\n",
    "# #score_test = knn.score(X_test_pca, y_test)\n",
    "\n",
    "# # Print result\n",
    "# print(f\"Training result: {score_train}\")\n",
    "# #print(f\"Test result: {score_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "285351e5d04eaa7ba8fcc836792d3255e07e50ee2507db40f71c874eba2e22ab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
