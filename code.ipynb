{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: TM10007 - Machine learning\n",
    "Editors: Lishia Vergeer, Amy Roos, Maaike Pruijt, Hilde Roording.\n",
    "\n",
    "Description: The aim of this code is to predict the tumor grade of gliomaâ€™s(high or low) before surgery, \n",
    "based on features extracted from a combination of four MRI images: \n",
    "T2-weighted, T2-weighted FLAIR and T1-weighted before and after injection of contrast agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "from sklearn import decomposition\n",
    "import seaborn\n",
    "\n",
    "# Import code\n",
    "from brats.load_data import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# scaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn import feature_selection \n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data\n",
    "- Bepalen wat test_size wordt. \n",
    "- output bij train_test split functie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_brats):\n",
    "\n",
    "    \"\"\"\n",
    "    This function creates a panda dataframe and splits the data into test and train components.\n",
    "    This is done with test_size variable and the function train_test_split from the sklearn module.\n",
    "    Returns a train set with the data of 55% and a test set of 45% of the subjects.\n",
    "    \"\"\"\n",
    "\n",
    "    data_features = pd.DataFrame(data=data_brats)\n",
    "    data_train, data_test = train_test_split(data_features, test_size=0.45) # Nog bepalen wat test_size wordt\n",
    "    #print(f'data_train: {data_train}')\n",
    "    #print(f'data_test: {data_test}')\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No None\n",
    "- Bepalen waar threshold ligt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_none(data):\n",
    "    '''\n",
    "    Deleting columns with NaN or filling them.\n",
    "    '''\n",
    "    # Inzicht in data\n",
    "    print(f'OVERZICHT: {data.isnull().sum()}')\n",
    "\n",
    "    # If the total number of NaN observations in a column are greater than 40%, delete the entire column.\n",
    "    perc = 40.0\n",
    "    min_count = int(((100-perc)/100)*data.shape[0] + 1)\n",
    "    data_dropcolumn = data.dropna(axis=1, thresh=min_count)\n",
    "    #print(data_dropcolumn)\n",
    "    #print(data_dropcolumn.size)\n",
    "\n",
    "    # fill the NaN observations.\n",
    "    data_fill = data_dropcolumn.fillna(data_dropcolumn.median()) #Bekijken mean of median\n",
    "    #print(data_fill)\n",
    "    #print(data_fill.size)\n",
    "\n",
    "    # Inzicht in data\n",
    "    print(f'OVERZICHT NONONE: {data_fill.isnull().sum()}')\n",
    "    return data_fill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data in X and y\n",
    "- Checken of splitten van data en labels inderdaad moet na het er uithalen van NaN's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(data_no_none):\n",
    "    '''\n",
    "    Split in X (data) and y (label)\n",
    "    '''\n",
    "    y = data_no_none.pop('label')\n",
    "    X = data_no_none\n",
    "    return y, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale features\n",
    "- Checken of Robust Scaler juiste keuze is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scale(data_train):\n",
    "    '''\n",
    "    Scale features\n",
    "    '''\n",
    "    # standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data_train)\n",
    "    X_scaled = scaler.transform(data_train)\n",
    "    print(X_scaled)\n",
    "    \n",
    "    # minmax scaler\n",
    "    scaler_two = MinMaxScaler()\n",
    "    scaler_two.fit(data_train)\n",
    "    X_scaled_two = scaler_two.transform(data_train)\n",
    "    print(X_scaled_two)\n",
    "    \n",
    "    # robustscaler\n",
    "    scaler_three = RobustScaler()\n",
    "    scaler_three.fit(data_train)\n",
    "    X_scaled_three = scaler_three.transform(data_train)\n",
    "    print(X_scaled_three)\n",
    "    return X_scaled_three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform features\n",
    "- We denken alleen PCA te gebruiken. Klopt het dat je dan niet ook selectie gebruikt?\n",
    "- PCA gaat uit van lineair model. Hoe kunnen we weten of ons onze data daar geschikt voor is?\n",
    "- Is het de bedoeling dat we ons hier verder in verdiepen of valt dat buiten de scope van het vak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transform(X_train, X_test, y_test):\n",
    "    '''\n",
    "    Transformation of features (PCA)\n",
    "    '''\n",
    "    # Perform a PCA\n",
    "    pca = decomposition.PCA(n_components=2)\n",
    "    pca.fit(X_train) \n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Fit kNN\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=15)\n",
    "    knn.fit(X_train_pca, y_train)\n",
    "    score_train = knn.score(X_train_pca, y_train)\n",
    "    score_test = knn.score(X_test_pca, y_test)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"Training result: {score_train}\")\n",
    "    print(f\"Test result: {score_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling functions\n",
    "- Is het het netst om in functies te werken of is dat niet de bedoeling aangezien we met yupiter notebook werken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_brats = load_data() \n",
    "    data_train, data_test = split_data(data_brats)\n",
    "    data_no_none_train = no_none(data_train)\n",
    "    y_train, X_train = split_xy(data_no_none_train)\n",
    "    #X_scale = feature_scale(X_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "285351e5d04eaa7ba8fcc836792d3255e07e50ee2507db40f71c874eba2e22ab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
