{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: TM10007 - Machine learning\n",
    "Editors: Lishia Vergeer, Amy Roos, Maaike Pruijt, Hilde Roording.\n",
    "\n",
    "Description: The aim of this code is to predict the tumor grade of gliomaâ€™s(high or low) before surgery, \n",
    "based on features extracted from a combination of four MRI images: \n",
    "T2-weighted, T2-weighted FLAIR and T1-weighted before and after injection of contrast agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "from sklearn import decomposition\n",
    "import seaborn\n",
    "\n",
    "# Import code\n",
    "from brats.load_data import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# scaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn import feature_selection \n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data\n",
    "This function creates a panda dataframe and splits the data into test and train components.\n",
    "This is done with test_size variable and the function train_test_split from the sklearn module.\n",
    "Returns a train set with the data of 55% and a test set of 45% of the subjects.\n",
    "- Bepalen wat test_size wordt. \n",
    "- output bij train_test split functie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = pd.DataFrame(data=data_brats)\n",
    "data_train, data_test = train_test_split(data_features, test_size=0.45) # Nog bepalen wat test_size wordt\n",
    "#print(f'data_train: {data_train}')\n",
    "#print(f'data_test: {data_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No None\n",
    "Deleting columns with NaN or filling them.\n",
    "- Bepalen waar threshold ligt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERZICHT: VOLUME_ET       0\n",
      "VOLUME_NET      0\n",
      "VOLUME_ED       0\n",
      "VOLUME_TC       0\n",
      "VOLUME_WT       0\n",
      "               ..\n",
      "TGM_Cog_X_6    91\n",
      "TGM_Cog_Y_6    91\n",
      "TGM_Cog_Z_6    91\n",
      "TGM_T_6        91\n",
      "label           0\n",
      "Length: 725, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lishia\\AppData\\Local\\Temp\\ipykernel_24508\\1875792392.py:12: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  data_fill = data_dropcolumn.fillna(data_dropcolumn.median()) #Bekijken mean of median\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERZICHT NONONE: VOLUME_ET      0\n",
      "VOLUME_NET     0\n",
      "VOLUME_ED      0\n",
      "VOLUME_TC      0\n",
      "VOLUME_WT      0\n",
      "              ..\n",
      "TGM_Cog_X_1    0\n",
      "TGM_Cog_Y_1    0\n",
      "TGM_Cog_Z_1    0\n",
      "TGM_T_1        0\n",
      "label          0\n",
      "Length: 705, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inzicht in data\n",
    "print(f'OVERZICHT: {data_train.isnull().sum()}')\n",
    "\n",
    "# If the total number of NaN observations in a column are greater than 40%, delete the entire column.\n",
    "perc = 40.0\n",
    "min_count = int(((100-perc)/100)*data_train.shape[0] + 1)\n",
    "data_dropcolumn = data_train.dropna(axis=1, thresh=min_count)\n",
    "#print(data_dropcolumn)\n",
    "#print(data_dropcolumn.size)\n",
    "\n",
    "# fill the NaN observations.\n",
    "data_fill = data_dropcolumn.fillna(data_dropcolumn.median()) #Bekijken mean of median\n",
    "#print(data_fill)\n",
    "#print(data_fill.size)\n",
    "\n",
    "# Inzicht in data\n",
    "print(f'OVERZICHT NONONE: {data_fill.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data in X and y\n",
    "Split in X (data) and y (label)\n",
    "- Checken of splitten van data en labels inderdaad moet na het er uithalen van NaN's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_fill.pop('label')\n",
    "X = data_fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale features\n",
    "- Checken of Robust Scaler juiste keuze is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.02947975 -0.39842764 -0.82184863 ... -1.91960116 -0.76537983\n",
      "  -1.11753984]\n",
      " [-1.00084585  1.59756712 -1.01725672 ... -0.03183738  0.81315031\n",
      "   0.67351092]\n",
      " [-0.47894876 -0.72296177  1.01502196 ... -1.93942162 -0.07950401\n",
      "  -0.79298959]\n",
      " ...\n",
      " [ 1.48695824 -0.59660762  1.59513821 ...  1.50231241 -0.30583326\n",
      "   0.14916238]\n",
      " [ 2.74335277 -0.05245009 -0.06716059 ...  0.76444149  0.73233359\n",
      "   1.19772214]\n",
      " [ 0.39527195 -0.29928273 -0.33423967 ...  1.77656263  0.3226358\n",
      "   0.22285711]]\n",
      "[[0.00118861 0.07596838 0.16121198 ... 0.00483228 0.2350806  0.03067322]\n",
      " [0.00833291 0.46039502 0.11608423 ... 0.46507421 0.59978766 0.37616523]\n",
      " [0.13854888 0.01346342 0.58542079 ... 0.         0.39354685 0.09327866]\n",
      " ...\n",
      " [0.62905265 0.03779911 0.71939344 ... 0.83910409 0.34125525 0.27501889]\n",
      " [0.94252946 0.14260332 0.33550042 ... 0.65920916 0.58111559 0.47728509]\n",
      " [0.35667139 0.09506359 0.2738209  ... 0.90596704 0.48645812 0.28923454]]\n",
      "[[-0.53644033 -0.05428053 -0.4690925  ... -1.20910833 -0.42434121\n",
      "  -0.76759156]\n",
      " [-0.51544208  2.41198921 -0.62032289 ...  0.01515955  0.60464909\n",
      "   0.79354754]\n",
      " [-0.13271639 -0.45527793  0.95249986 ... -1.22196245  0.02275799\n",
      "  -0.48470282]\n",
      " ...\n",
      " [ 1.30895306 -0.29915356  1.40146389 ...  1.01009882 -0.12477838\n",
      "   0.33650801]\n",
      " [ 2.23031181  0.37321256  0.11497627 ...  0.53156879  0.55196741\n",
      "   1.25046738]\n",
      " [ 0.50838072  0.06822384 -0.09172178 ...  1.1879578   0.2848993\n",
      "   0.40074277]]\n"
     ]
    }
   ],
   "source": [
    "# standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled_one = scaler.transform(X)\n",
    "print(X_scaled_one)\n",
    "\n",
    "# minmax scaler\n",
    "scaler_two = MinMaxScaler()\n",
    "scaler_two.fit(X)\n",
    "X_scaled_two = scaler_two.transform(X)\n",
    "print(X_scaled_two)\n",
    "\n",
    "# robustscaler\n",
    "scaler_three = RobustScaler()\n",
    "scaler_three.fit(X)\n",
    "X_scaled = scaler_three.transform(X)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform features\n",
    "- We denken alleen PCA te gebruiken. Klopt het dat je dan niet ook selectie gebruikt?\n",
    "- PCA gaat uit van lineair model. Hoe kunnen we weten of ons onze data daar geschikt voor is?\n",
    "- Is het de bedoeling dat we ons hier verder in verdiepen of valt dat buiten de scope van het vak?\n",
    "- Uitzoeken hoe we de X_test en y_test correct gereed krijgen voor PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_test_pca = pca.transform(X_test)\\n\\n# Fit kNN\\nknn = neighbors.KNeighborsClassifier(n_neighbors=15)\\nknn.fit(X_train_pca, y_train)\\nscore_train = knn.score(X_train_pca, y_train)\\nscore_test = knn.score(X_test_pca, y_test)\\n\\n# Print result\\nprint(f\"Training result: {score_train}\")\\nprint(f\"Test result: {score_test}\")\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a PCA\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X_scaled) \n",
    "X_train_pca = pca.transform(X_scaled)\n",
    "\n",
    "'''\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Fit kNN\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "score_train = knn.score(X_train_pca, y_train)\n",
    "score_test = knn.score(X_test_pca, y_test)\n",
    "\n",
    "# Print result\n",
    "print(f\"Training result: {score_train}\")\n",
    "print(f\"Test result: {score_test}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "285351e5d04eaa7ba8fcc836792d3255e07e50ee2507db40f71c874eba2e22ab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
